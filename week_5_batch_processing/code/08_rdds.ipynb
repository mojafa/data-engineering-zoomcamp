{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c64b680",
   "metadata": {},
   "source": [
    "What is RDD and how is it related to dataframe\n",
    "From DataFrame to RDD\n",
    "Operators on RDDs: map, filter, reduceByKey\n",
    "From RDD to DataFrame\n",
    "\n",
    "Resilient Distributed Datasets (RDDs) are the main abstraction provided by Spark and consist of collection of elements partitioned accross the nodes of the cluster.\n",
    "\n",
    "Dataframes are actually built on top of RDDs and contain a schema as well, which plain RDDs do not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d66f42fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/09 20:36:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "646fc343",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_green = spark.read.parquet('data/pq/green/*/*')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196cccd5",
   "metadata": {},
   "source": [
    "```\n",
    "SELECT \n",
    "    date_trunc('hour', lpep_pickup_datetime) AS hour, \n",
    "    PULocationID AS zone,\n",
    "\n",
    "    SUM(total_amount) AS amount,\n",
    "    COUNT(1) AS number_records\n",
    "FROM\n",
    "    green\n",
    "WHERE\n",
    "    lpep_pickup_datetime >= '2020-01-01 00:00:00'\n",
    "GROUP BY\n",
    "    1, 2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74fe52cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rdd = df_green \\\n",
    "    .select('lpep_pickup_datetime', 'PULocationID', 'total_amount') \\\n",
    "    .rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85d957f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.types.Row'>\n"
     ]
    }
   ],
   "source": [
    "print(type(df_green.rdd.take(1)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc68456f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 28, 9, 54, 40), lpep_dropoff_datetime=datetime.datetime(2020, 1, 28, 10, 6, 19), store_and_fwd_flag='N', RatecodeID=1, PULocationID=92, DOLocationID=82, passenger_count=1, trip_distance=4.08, fare_amount=14.5, extra=0.0, mta_tax=0.5, tip_amount=1.5, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=16.8, payment_type=1, trip_type=1, congestion_surcharge=0.0),\n",
       " Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 8, 20, 11, 55), lpep_dropoff_datetime=datetime.datetime(2020, 1, 8, 20, 14, 35), store_and_fwd_flag='N', RatecodeID=1, PULocationID=189, DOLocationID=181, passenger_count=1, trip_distance=0.56, fare_amount=4.0, extra=0.5, mta_tax=0.5, tip_amount=0.0, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=5.3, payment_type=2, trip_type=1, congestion_surcharge=0.0),\n",
       " Row(VendorID=None, lpep_pickup_datetime=datetime.datetime(2020, 1, 10, 11, 23), lpep_dropoff_datetime=datetime.datetime(2020, 1, 10, 11, 26), store_and_fwd_flag=None, RatecodeID=None, PULocationID=196, DOLocationID=196, passenger_count=None, trip_distance=0.05, fare_amount=13.16, extra=0.0, mta_tax=0.0, tip_amount=0.0, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=13.46, payment_type=None, trip_type=None, congestion_surcharge=None)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_green.rdd.take(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a0bf382",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa2b00f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime(year=2020, month=1, day=1)\n",
    "\n",
    "def filter_outliers(row):\n",
    "    return row.lpep_pickup_datetime >= start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69dd326d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rows = rdd.take(10)\n",
    "row = rows[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd4b7006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 28, 9, 54, 40), PULocationID=92, total_amount=16.8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A Row is a special object pyspark.sql.types.Row.\n",
    "\n",
    "row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cfe858",
   "metadata": {},
   "source": [
    "# Operations on RDDs: \n",
    "## filter, map, reduceByKey, map, toDF\n",
    "### .filter() because we don’t want outliers\n",
    "### .map() to generate intermediate results better suited for aggregation\n",
    "### .reduceByKey() to merge the values for each key\n",
    "### .map() to unwrap the rows\n",
    "### .toDF() to return the rows to a dataframe properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcb3c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This returns the first row.\n",
    "rdd.filter(lambda row: True).take(1)\n",
    "\n",
    "# This filters the while dataset and returns an empty list.\n",
    "rdd.filter(lambda row: False).take(1)\n",
    "\n",
    "# This returns even numbers.\n",
    "# See https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.RDD.filter.html\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "rdd.filter(lambda x: x % 2 == 0).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7c192e",
   "metadata": {},
   "source": [
    "### We will use this to filter a Timestamp.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0be16f35",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (3663632722.py, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/kt/s8b94f056s9f7ljl6s_gz7th0000gn/T/ipykernel_95822/3663632722.py\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    .take(3)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "start = datetime(year=2020, month=1, day=1)\n",
    "\n",
    "rdd.filter(lambda row: riw.lpep_pickup_datetime >= start).take(5)\n",
    "\n",
    "# Better, because with lambda we gets messy quiet fast.\n",
    "def filter_outliers(row):\n",
    "    return row.lpep_pickup_datetime >= start\n",
    "\n",
    "rdd \\\n",
    "    .filter(filter_outliers)\n",
    "    .take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb64850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27cab82d",
   "metadata": {},
   "source": [
    "## To implement the equivalent of GroupBy, we need the .map() function. A map() applied a transformation to every Row and return a transformed RDD.\n",
    "\n",
    "We create a function that transforms a row by creating a tuple composed of a key and a value. The key is a tuple of hour and zone, the same two columns of the GROUP BY. The value is a tuple of amount and number of records, the same two columns that are returned by the SQL query above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d99eb089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((datetime.datetime(2020, 1, 28, 9, 0), 92), (16.8, 1)),\n",
       " ((datetime.datetime(2020, 1, 8, 20, 0), 189), (5.3, 1)),\n",
       " ((datetime.datetime(2020, 1, 10, 11, 0), 196), (13.46, 1)),\n",
       " ((datetime.datetime(2020, 1, 23, 15, 0), 260), (6.3, 1)),\n",
       " ((datetime.datetime(2020, 1, 17, 10, 0), 197), (10.8, 1))]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_for_grouping(row): \n",
    "    hour = row.lpep_pickup_datetime.replace(minute=0, second=0, microsecond=0)\n",
    "    zone = row.PULocationID\n",
    "    key = (hour, zone)\n",
    "    \n",
    "    amount = row.total_amount\n",
    "    count = 1\n",
    "    value = (amount, count)\n",
    "\n",
    "    return (key, value)\n",
    "rdd \\\n",
    "    .filter(filter_outliers) \\\n",
    "    .map(prepare_for_grouping) \\\n",
    "    .take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf9f242",
   "metadata": {},
   "source": [
    ".reduceByKey()\n",
    " (5.5.1) Reduce by key\n",
    "\n",
    "Now, we will aggregate this RDD transformed RDD by the key.\n",
    "\n",
    "To do so, we will use pyspark.RDD.reduceByKey to merge the values for each key using an associative and commutative reduce function.\n",
    "\n",
    "Here, left_value and rigth_value are tuple of amount and number of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb328a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:===========================================>              (3 + 1) / 4]\r"
     ]
    }
   ],
   "source": [
    "def calculate_revenue(left_value, right_value):\n",
    "    left_amount, left_count = left_value\n",
    "    right_amount, right_count = right_value\n",
    "    \n",
    "    output_amount = left_amount + right_amount\n",
    "    output_count = left_count + right_count\n",
    "    \n",
    "    return (output_amount, output_count)\n",
    "df_result = rdd \\\n",
    "    .filter(filter_outliers) \\\n",
    "    .map(prepare_for_grouping) \\\n",
    "    .reduceByKey(calculate_revenue) \\\n",
    "    .take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682015b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The nested Row structure isn’t really nice to use, and we want to fall back to a data frame.\n",
    "\n",
    "# To do so, we apply another map function to transform the rows into the desired columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ea260f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dae6064",
   "metadata": {},
   "outputs": [],
   "source": [
    "RevenueRow = namedtuple('RevenueRow', ['hour', 'zone', 'revenue', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0a98ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unwrap(row):\n",
    "    return RevenueRow(\n",
    "        hour=row[0][0], \n",
    "        zone=row[0][1],\n",
    "        revenue=row[1][0],\n",
    "        count=row[1][1]\n",
    "    )\n",
    "rdd \\\n",
    "    .filter(filter_outliers) \\\n",
    "    .map(prepare_for_grouping) \\\n",
    "    .reduceByKey(calculate_revenue) \\\n",
    "    .map(unwrap) \\\n",
    "    .take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4e05d3",
   "metadata": {},
   "source": [
    "### Returning to a dataframe\n",
    "To return to a dataframe properly, we want to fix the schema.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a09200b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c14d15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_schema = types.StructType([\n",
    "    types.StructField('hour', types.TimestampType(), True),\n",
    "    types.StructField('zone', types.IntegerType(), True),\n",
    "    types.StructField('revenue', types.DoubleType(), True),\n",
    "    types.StructField('count', types.IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e376304",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = rdd \\\n",
    "    .filter(filter_outliers) \\\n",
    "    .map(prepare_for_grouping) \\\n",
    "    .reduceByKey(calculate_revenue) \\\n",
    "    .map(unwrap) \\\n",
    "    .toDF(result_schema)\n",
    "\n",
    "df_result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96803203",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4675bd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_result.write.parquet('tmp/green-revenue')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bf0d65",
   "metadata": {},
   "source": [
    "## mapPartitions() returns a new RDD by applying a function to each partition of this RDD.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "255b5503",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['VendorID', 'lpep_pickup_datetime', 'PULocationID', 'DOLocationID', 'trip_distance']\n",
    "\n",
    "duration_rdd = df_green \\\n",
    "    .select(columns) \\\n",
    "    .rdd\n",
    "duration_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5678c6",
   "metadata": {},
   "source": [
    "## Below is a two simples codes that helps to understand how mapPartitions() works.\n",
    "\n",
    "The code below returns [1] for each partitions and flattens the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27aae48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model_in_batch(partition):\n",
    "    return [1]\n",
    "\n",
    "rdd.mapPartitions(apply_model_in_batch).collect()\n",
    "# [1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3e3658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model_in_batch(partition):\n",
    "    cnt = 0\n",
    "\n",
    "    for row in partition:\n",
    "        cnt = cnt + 1\n",
    "\n",
    "    return [cnt]\n",
    "\n",
    "rdd.mapPartitions(apply_model_in_batch).collect()\n",
    "# [1141148, 436983, 433476, 292910]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cd64b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def apply_model_in_batch(rows):\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "    cnt = len(df)\n",
    "    return [cnt]\n",
    "\n",
    "duration_rdd.mapPartitions(apply_model_in_batch).collect()\n",
    "# [1141148, 436983, 433476, 292910]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d686c7a",
   "metadata": {},
   "source": [
    "## It put the entire partition in a dataframe, which isn’t always good. If you want to solve it, you can use .islice() to break a partition into 100,000 rows subpartitions and treat them them separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "921e4ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows = duration_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f50db3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(rows, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8ecc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ...\n",
    "\n",
    "def model_predict(df):\n",
    "    # y_pred = model.predict(df)\n",
    "    y_pred = df.trip_distance * 5\n",
    "    return y_pred\n",
    "\n",
    "def apply_model_in_batch(rows):\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "    predictions = model_predict(df)\n",
    "    df['predicted_duration'] = predictions\n",
    "\n",
    "    for row in df.itertuples():\n",
    "        yield row\n",
    "\n",
    "df_predicts = duration_rdd \\\n",
    "    .mapPartitions(apply_model_in_batch) \\\n",
    "    .toDF() \\\n",
    "    .drop('Index')\n",
    "\n",
    "df_predicts.select('predicted_duration').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b64641",
   "metadata": {},
   "source": [
    "Here, itertuples() iterates over DataFrame rows as namedtuples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caecca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = duration_rdd.take(5)\n",
    "df = pd.DataFrame(rows, columns=columns)\n",
    "list(df.itertuples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ba03d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851af825",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4c9b44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fffc66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faf3ae3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
