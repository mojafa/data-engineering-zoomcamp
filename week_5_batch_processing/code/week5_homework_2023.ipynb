{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5155d831",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "### Install Spark and PySpark\n",
    "\n",
    "#### Install Spark\n",
    "#### Run PySpark\n",
    "#### Create a local spark session\n",
    "#### Execute spark.version.\n",
    "<!-- #### What’s the output?\n",
    "\n",
    "3.3.2\n",
    "2.1.4\n",
    "1.2.3\n",
    "5.4\n",
    " -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34e99ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/09 19:49:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/03/09 19:49:34 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3.3.2'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7411cf6",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "###### HVFHW June 2021\n",
    "\n",
    "###### Read it with Spark using the same schema as we did in the lessons.\n",
    "###### We will use this dataset for all the remaining questions.\n",
    "\n",
    "###### Repartition it to 12 partitions and save it to parquet.\n",
    "\n",
    "###### What is the average size of the Parquet (ending with .parquet extension) Files that were created (in MB)?\n",
    "\n",
    "###### Select the answer which most closely matches.\n",
    "\n",
    "###### 2 MB\n",
    "###### 24 MB\n",
    "###### 100 MB\n",
    "###### 250 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88f34208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-09 19:28:06--  https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhvhv/fhvhv_tripdata_2021-06.csv.gz\n",
      "Resolving github.com (github.com)... 140.82.121.3\n",
      "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/4564ad9e-a6da-4923-ad6f-35ff02446a51?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230309%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230309T172806Z&X-Amz-Expires=300&X-Amz-Signature=a279ac5c421ccfb4108608625f33a27c282faa6d55e7c1afbfaaf865ca6040cf&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dfhvhv_tripdata_2021-06.csv.gz&response-content-type=application%2Foctet-stream [following]\n",
      "--2023-03-09 19:28:07--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/4564ad9e-a6da-4923-ad6f-35ff02446a51?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230309%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230309T172806Z&X-Amz-Expires=300&X-Amz-Signature=a279ac5c421ccfb4108608625f33a27c282faa6d55e7c1afbfaaf865ca6040cf&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dfhvhv_tripdata_2021-06.csv.gz&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 175799316 (168M) [application/octet-stream]\n",
      "Saving to: 'fhvhv_tripdata_2021-06.csv.gz'\n",
      "\n",
      "fhvhv_tripdata_2021 100%[===================>] 167.66M  9.90MB/s    in 21s     \n",
      "\n",
      "2023-03-09 19:28:28 (8.16 MB/s) - 'fhvhv_tripdata_2021-06.csv.gz' saved [175799316/175799316]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhvhv/fhvhv_tripdata_2021-06.csv.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0f0b5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 jafa  staff   878M Dec 20 02:13 fhvhv_tripdata_2021-06.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gzip: fhvhv_tripdata_2021-06.csv: unknown suffix -- ignored\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "gzip -d fhvhv_tripdata_2021-06.csv\n",
    "ls -lh fhvhv_tripdata_2021-06.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39771551",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "schema = types.StructType([\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PULocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "    types.StructField('SR_Flag', types.StringType(), True),\n",
    "    types.StructField('Affiliated_base_number', types.StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4f611e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('fhvhv_tripdata_2021-06.csv')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e3149f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 11 fhvhv_tripdata_2021-06.csv > head.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0929d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dispatching_base_num,pickup_datetime,dropoff_datetime,PULocationID,DOLocationID,SR_Flag,Affiliated_base_number\r\n",
      "B02764,2021-06-01 00:02:41,2021-06-01 00:07:46,174,18,N,B02764\r\n",
      "B02764,2021-06-01 00:16:16,2021-06-01 00:21:14,32,254,N,B02764\r\n",
      "B02764,2021-06-01 00:27:01,2021-06-01 00:42:11,240,127,N,B02764\r\n",
      "B02764,2021-06-01 00:46:08,2021-06-01 00:53:45,127,235,N,B02764\r\n",
      "B02510,2021-06-01 00:45:42,2021-06-01 01:03:33,144,146,N,\r\n",
      "B02510,2021-06-01 00:18:15,2021-06-01 00:25:47,49,17,N,\r\n",
      "B02510,2021-06-01 00:33:06,2021-06-01 00:42:46,49,225,N,\r\n",
      "B02510,2021-06-01 00:46:27,2021-06-01 00:56:50,225,177,N,\r\n",
      "B02764,2021-06-01 00:48:06,2021-06-01 01:04:10,209,45,N,B02764\r\n",
      "B02875,2021-06-01 00:18:54,2021-06-01 00:26:14,80,256,N,B02875\r\n"
     ]
    }
   ],
   "source": [
    "!cat head.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a07be428",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0e616fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.parquet('data/pq/fhvhv/2021/06/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dead1d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 566784\r\n",
      "-rw-r--r--  1 jafa  staff     0B Mar  9 19:52 _SUCCESS\r\n",
      "-rw-r--r--  1 jafa  staff    23M Mar  9 19:52 part-00000-20148f5e-0884-468f-a234-e2ea82b19eb2-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 jafa  staff    23M Mar  9 19:52 part-00001-20148f5e-0884-468f-a234-e2ea82b19eb2-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 jafa  staff    23M Mar  9 19:52 part-00002-20148f5e-0884-468f-a234-e2ea82b19eb2-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 jafa  staff    23M Mar  9 19:52 part-00003-20148f5e-0884-468f-a234-e2ea82b19eb2-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 jafa  staff    23M Mar  9 19:52 part-00004-20148f5e-0884-468f-a234-e2ea82b19eb2-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 jafa  staff    23M Mar  9 19:52 part-00005-20148f5e-0884-468f-a234-e2ea82b19eb2-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 jafa  staff    23M Mar  9 19:52 part-00006-20148f5e-0884-468f-a234-e2ea82b19eb2-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 jafa  staff    23M Mar  9 19:52 part-00007-20148f5e-0884-468f-a234-e2ea82b19eb2-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 jafa  staff    23M Mar  9 19:52 part-00008-20148f5e-0884-468f-a234-e2ea82b19eb2-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 jafa  staff    23M Mar  9 19:52 part-00009-20148f5e-0884-468f-a234-e2ea82b19eb2-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 jafa  staff    23M Mar  9 19:52 part-00010-20148f5e-0884-468f-a234-e2ea82b19eb2-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 jafa  staff    23M Mar  9 19:52 part-00011-20148f5e-0884-468f-a234-e2ea82b19eb2-c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh data/pq/fhvhv/2021/06/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e559f76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b88cd1b0",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "###### Count records\n",
    "\n",
    "###### How many taxi trips were there on June 15?\n",
    "###### Consider only trips that started on June 15.\n",
    "\n",
    "###### 308,164\n",
    "###### 12,856\n",
    "###### 452,470\n",
    "###### 50,982"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee15c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('data/pq/fhvhv/2021/06/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec61db4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8aea7727",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "452470"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .filter(\"pickup_date = '2021-06-15'\") \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12adf6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:==========================================>                (5 + 2) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  452470|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView('fhvhv_2021_06')\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    COUNT(1)\n",
    "FROM\n",
    "    fhvhv_2021_06\n",
    "WHERE\n",
    "    to_date(pickup_datetime) = '2021-06-15';\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d646da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8a322b9",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "###### Longest trip for each day\n",
    "\n",
    "###### Now calculate the duration for each trip.\n",
    "###### How long was the longest trip in Hours?\n",
    "\n",
    "66.87 Hours\n",
    "243.44 Hours\n",
    "7.68 Hours\n",
    "3.32 Hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8b1987e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dispatching_base_num',\n",
       " 'pickup_datetime',\n",
       " 'dropoff_datetime',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'SR_Flag',\n",
       " 'Affiliated_base_number']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea76b527",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:===================================================>    (11 + 1) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|pickup_date|     max(duration)|\n",
      "+-----------+------------------+\n",
      "| 2021-06-25| 66.87888888888888|\n",
      "| 2021-06-22|25.549722222222222|\n",
      "| 2021-06-27|19.980833333333333|\n",
      "| 2021-06-26| 18.19722222222222|\n",
      "| 2021-06-23|16.466944444444444|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.withColumn('duration', (df.dropoff_datetime.cast('long') - df.pickup_datetime.cast('long'))/60/60) \\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .groupBy('pickup_date') \\\n",
    "        .max('duration') \\\n",
    "    .orderBy('max(duration)', ascending=False) \\\n",
    "    .limit(5) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "122066c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:======================================>                  (8 + 4) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|pickup_date|          duration|\n",
      "+-----------+------------------+\n",
      "| 2021-06-25| 66.87888888888888|\n",
      "| 2021-06-22|25.549722222222222|\n",
      "| 2021-06-27|19.980833333333333|\n",
      "| 2021-06-26| 18.19722222222222|\n",
      "| 2021-06-23|16.466944444444444|\n",
      "| 2021-06-24|13.909722222222223|\n",
      "| 2021-06-04|             11.67|\n",
      "| 2021-06-20|10.984444444444446|\n",
      "| 2021-06-01|           10.2675|\n",
      "| 2021-06-28|  9.96638888888889|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 23:===================================================>    (11 + 1) / 12]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    to_date(pickup_datetime) AS pickup_date,\n",
    "    MAX((CAST(dropoff_datetime AS LONG) - CAST(pickup_datetime AS LONG)) / 60 / 60) AS duration\n",
    "FROM\n",
    "    fhvhv_2021_06\n",
    "GROUP BY\n",
    "    1\n",
    "ORDER BY\n",
    "    2 DESC\n",
    "LIMIT 10;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53d8d4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc6e9ea5",
   "metadata": {},
   "source": [
    "# Question 5\n",
    "## User Interface\n",
    "\n",
    "### Spark’s User Interface which shows application’s dashboard runs on which local port?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d9036e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binds to ports 4040 and if multiple SparkConetxts are running on the same host, they bind to sucessive ports begining from 4040 up wards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3ff207",
   "metadata": {},
   "source": [
    "# Question 6\n",
    "## Most frequent pickup location zone\n",
    "\n",
    "### Load the zone lookup data into a temp view in Spark\n",
    "### Zone Data.\n",
    "\n",
    "#### Using the zone lookup data and the fhvhv June 2021 data, what is the name of the most frequent pickup location zone?\n",
    "\n",
    "###### East Chelsea, Astoria, Union Sq, Crown Heights North"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fca127bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LocationID', 'Borough', 'Zone', 'service_zone']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zones = spark.read.parquet('zones')\n",
    "df_zones.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bed60be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dispatching_base_num',\n",
       " 'pickup_datetime',\n",
       " 'dropoff_datetime',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'SR_Flag',\n",
       " 'Affiliated_base_number']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a319e60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones.createOrReplaceTempView(\"zones\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6e74c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT zones.Zone, COUNT(1)\n",
    "FROM\n",
    "    fhvhv_2021_06 fhv\n",
    "    LEFT JOIN zones ON fhv.PULocationID = zones.LocationID\n",
    "GROUP BY 1\n",
    "ORDER BY 2 DESC\n",
    "LIMIT 5;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36acdfbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
