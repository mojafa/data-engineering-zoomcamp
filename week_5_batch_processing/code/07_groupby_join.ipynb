{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4341e0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/09 20:19:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd304aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_green = spark.read.parquet('data/pq/green/*/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "243991f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jafa/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py:229: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df_green.registerTempTable('green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e43764a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green_revenue = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    date_trunc('hour', lpep_pickup_datetime) AS hour, \n",
    "    PULocationID AS zone,\n",
    "\n",
    "    SUM(total_amount) AS amount,\n",
    "    COUNT(1) AS number_records\n",
    "FROM\n",
    "    green\n",
    "WHERE\n",
    "    lpep_pickup_datetime >= '2020-01-01 00:00:00'\n",
    "GROUP BY\n",
    "    1, 2\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e00310e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_green_revenue \\\n",
    "    .repartition(20) \\\n",
    "    .write.parquet('data/report/revenue/green', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07ebb68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow = spark.read.parquet('data/pq/yellow/*/*')\n",
    "df_yellow.registerTempTable('yellow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d5be29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow_revenue = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    date_trunc('hour', tpep_pickup_datetime) AS hour, \n",
    "    PULocationID AS zone,\n",
    "\n",
    "    SUM(total_amount) AS amount,\n",
    "    COUNT(1) AS number_records\n",
    "FROM\n",
    "    yellow\n",
    "WHERE\n",
    "    tpep_pickup_datetime >= '2020-01-01 00:00:00'\n",
    "GROUP BY\n",
    "    1, 2\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bd9264e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_yellow_revenue \\\n",
    "    .repartition(20) \\\n",
    "    .write.parquet('data/report/revenue/yellow', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba714643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+------------------+--------------+\n",
      "|               hour|zone|            amount|number_records|\n",
      "+-------------------+----+------------------+--------------+\n",
      "|2021-01-27 17:00:00|  70|             153.2|             3|\n",
      "|2020-02-09 06:00:00| 229|            717.36|            38|\n",
      "|2021-06-06 11:00:00|  13|            588.95|            28|\n",
      "|2020-02-18 12:00:00| 259|             53.21|             2|\n",
      "|2020-10-04 12:00:00| 264| 359.3400000000001|            25|\n",
      "|2020-08-03 05:00:00| 151|             89.21|             6|\n",
      "|2020-03-08 16:00:00|  50|3427.0499999999993|           194|\n",
      "|2021-01-18 17:00:00|  79| 826.4999999999998|            58|\n",
      "|2021-06-20 02:00:00|   7|53.760000000000005|             4|\n",
      "|2020-08-22 17:00:00| 168|151.45000000000002|             4|\n",
      "+-------------------+----+------------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_yellow_revenue.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb7267b",
   "metadata": {},
   "source": [
    "### Joining two large tables\n",
    "##### Spark can join two tables quite easily. The syntax is easy to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd5d74d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 43:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+-----------------+--------------------+------------------+---------------------+\n",
      "|               hour|zone|     green_amount|green_number_records|     yellow_amount|yellow_number_records|\n",
      "+-------------------+----+-----------------+--------------------+------------------+---------------------+\n",
      "|2020-01-01 00:00:00|   4|             null|                null|1004.3000000000002|                   57|\n",
      "|2020-01-01 00:00:00|  14|             null|                null|               8.8|                    1|\n",
      "|2020-01-01 00:00:00|  24|             87.6|                   3| 754.9500000000002|                   45|\n",
      "|2020-01-01 00:00:00|  25|            531.0|                  26|            324.35|                   16|\n",
      "|2020-01-01 00:00:00|  33|317.2700000000001|                  11|            255.56|                    8|\n",
      "+-------------------+----+-----------------+--------------------+------------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_green_revenue = spark.read.parquet('data/report/revenue/green')\n",
    "df_yellow_revenue = spark.read.parquet('data/report/revenue/yellow')\n",
    "\n",
    "df_green_revenue_tmp = df_green_revenue \\\n",
    "    .withColumnRenamed('amount', 'green_amount') \\\n",
    "    .withColumnRenamed('number_records', 'green_number_records')\n",
    "\n",
    "df_yellow_revenue_tmp = df_yellow_revenue \\\n",
    "    .withColumnRenamed('amount', 'yellow_amount') \\\n",
    "    .withColumnRenamed('number_records', 'yellow_number_records')\n",
    "\n",
    "df_join = df_green_revenue_tmp.join(df_yellow_revenue_tmp, on=['hour', 'zone'], how='outer')\n",
    "\n",
    "# Materialized the result\n",
    "df_join.write.parquet('data/report/revenue/total', mode='overwrite')\n",
    "\n",
    "df_join.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35015ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the result previously created.\n",
    "df_join = spark.read.parquet('data/report/revenue/total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec9f34ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hour: timestamp (nullable = true)\n",
      " |-- zone: integer (nullable = true)\n",
      " |-- green_amount: double (nullable = true)\n",
      " |-- green_number_records: long (nullable = true)\n",
      " |-- yellow_amount: double (nullable = true)\n",
      " |-- yellow_number_records: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_join.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99e7318d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+------------------+--------------------+-----------------+---------------------+\n",
      "|               hour|zone|      green_amount|green_number_records|    yellow_amount|yellow_number_records|\n",
      "+-------------------+----+------------------+--------------------+-----------------+---------------------+\n",
      "|2020-01-01 00:00:00|  15|              null|                null|            34.09|                    1|\n",
      "|2020-01-01 00:00:00|  29|              61.3|                   1|             null|                 null|\n",
      "|2020-01-01 00:00:00|  40|168.98000000000002|                   8|            89.97|                    5|\n",
      "|2020-01-01 00:00:00|  42| 799.7599999999999|                  52|635.3500000000001|                   46|\n",
      "|2020-01-01 00:00:00|  45|              null|                null|732.4800000000001|                   42|\n",
      "+-------------------+----+------------------+--------------------+-----------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_join.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce247d09",
   "metadata": {},
   "source": [
    "## Read the zones.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "809c4276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-09 20:26:27--  https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.249.254, 52.217.14.62, 52.216.61.72, ...\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.249.254|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12322 (12K) [application/octet-stream]\n",
      "Saving to: 'taxi+_zone_lookup.csv.1'\n",
      "\n",
      "taxi+_zone_lookup.c 100%[===================>]  12.03K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2023-03-09 20:26:29 (9.90 MB/s) - 'taxi+_zone_lookup.csv.1' saved [12322/12322]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b15fa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", \"true\").csv('taxi+_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "efc30606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LocationID: string (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Zone: string (nullable = true)\n",
      " |-- service_zone: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df.write.parquet('zones') // already exists\n",
    "df_zones = spark.read.parquet('zones/')\n",
    "df_zones.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "abb46398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zones.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b3cf98a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = df_join.join(df_zones, df_join.zone == df_zones.LocationID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5e0614ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.drop('LocationID', 'zone').write.parquet('tmp/revenue-zones')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7d882d",
   "metadata": {},
   "source": [
    "###### What exactly Spark is doing\n",
    "Each executor processes a partition of Revenue DataFrame.\n",
    "Zones DataFrame is a small table.\n",
    "Because Zones is very small, each executor gets a copy of the entire Zones DataFrame and merges it with their partition of Revenue DataFrame within memory.\n",
    "Spark broadcast joins are perfect for joining a large DataFrame with a small DataFrame.\n",
    "Spark can \"broadcast\" a small DataFrame by sending all the data in that small DataFrame to all nodes in the cluster.\n",
    "After the small DataFrame is broadcasted, Spark can perform a join without shuffling any of the data in the large DataFrame.\n",
    "This is really (really!) fast."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
